{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":73291,"databundleVersionId":8930475,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"name":"THE BLEND","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Binary Classification of Insurance Cross Selling\n\n#### Playground Series - Season 4, Episode 7\n\n## Model Blend with CatBoost, LightGBM, and XGBoost.\n\n**Author:** Paulo Araya-Santiago\n\nWelcome to my comprehensive notebook for the Kaggle Playground Series:Binary Classification of Insurance Cross Selling. This notebook demonstrates my end-to-end workflow, from data exploration and preprocessing to feature engineering and model tuning. The goal is to build a robust model that accurately predicts insurance responses. This particular version of this notebook is meant to be ran in Kaggle and displayed for the competition.","metadata":{"id":"jvh6IYIuv7E9"}},{"cell_type":"markdown","source":"## Preliminary Tests and Findings\n\nThis notebook is the culmination of the creation of at least 50 different notebooks for this competition. As of the creation of this version I still haven't been able to obtain my ideal score, but I decided to share my favorite parts of my work. I started by testing the dataset on different models with only the necessary transformations done to the features. XGBoost and LightGBM stood out from the start. I settled on trying to perfect the LightGBM model by running it through hundreds of iterations across multiple weeks on OPTUNA. When I was satisfied with the validation scores, I submitted it to the competition and the results were lackluster. This led me to take a deep dive into other notebooks scoring high in the competition (all of which are referenced below). After trying all sorts of feature engineering techniques, through testing and research I found that the dataset worked best with the addition of a few interaction features. I later found a notebook along the highest scoring ones that created a blend out of other people's submissions. I found this interesting and decided to concuct my own blend. At first the blend created an averaged prediction through stratified KFolds but it was too computationally expensive, so I settled with three straight forward models.","metadata":{"id":"9lVR_MUgv7E-"}},{"cell_type":"markdown","source":"## Early Explorations\n\nI first started with a little exploratory data analysis (EDA) and basic data processing. The data comes pretty clean, but the dataset is MASSIVE. The largest I had worked with previously had been around 70k rows, but this one had 11 million. Not many columns though. Most columns were fairly easy to handle except for `Region_Code` and `Policy_Sales_Channel`. I treated those by binning them into a rare category due to the heavy imbalance towards some values. Otherwise, everything was treated pretty basically.\n\nI made some basic EDA graphs to explore the data, using some base models to understand feature importances. Later, I discovered the magic of KLIB from another notebook: [Optuna XGBoost KLIB Notebook](https://www.kaggle.com/code/suvroo/ps4e7-optuna-xgboost-klib), which taught me a thing or two about cleaning the data easily with KLIB, and how to keep track of hyperparameter studies with Optuna and some of its also amazing graphs.\n\nAt first, I applied the basic preprocessing steps necessary to run the dataset efficiently in the models I was using. Also incorporating some downcasting to the workflow so that the model would work more efficiently. I tried applying some additional transformations to the data like removing outliers, creating rare categories for feature values with low counts, and creating KMEANS cluster feature. I also tested borrowed feature interactions from other notebooks and created my own. After possibly hundreds of tests, I settled on a combination of my own feature interactions and a set of feature interactions from [this Kaggle notebook](https://www.kaggle.com/code/rohanrao/automl-grand-prix-1st-place-solution).","metadata":{"id":"gZfcudtGv7E-"}},{"cell_type":"markdown","source":"## Libraries Import\nIn this section, we import all necessary libraries required for data manipulation, visualization, model building, and evaluation.","metadata":{"id":"4V4Xo5xdv7E_"}},{"cell_type":"code","source":"%%capture\n!pip install klib","metadata":{"id":"8-w96LmFv7E_","execution":{"iopub.status.busy":"2024-07-30T15:41:15.819662Z","iopub.execute_input":"2024-07-30T15:41:15.820015Z","iopub.status.idle":"2024-07-30T15:41:30.132297Z","shell.execute_reply.started":"2024-07-30T15:41:15.819986Z","shell.execute_reply":"2024-07-30T15:41:30.131038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport warnings\nfrom datetime import datetime\nimport klib\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler, RobustScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import roc_auc_score\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, Pool\n\nfrom datetime import datetime\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"H2gvBOYGv7E_","execution":{"iopub.status.busy":"2024-07-30T15:41:30.134288Z","iopub.execute_input":"2024-07-30T15:41:30.134603Z","iopub.status.idle":"2024-07-30T15:41:33.159536Z","shell.execute_reply.started":"2024-07-30T15:41:30.134577Z","shell.execute_reply":"2024-07-30T15:41:33.158690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loading\nHere, we load the training dataset. This dataset will be used for all subsequent data processing and model training steps. This dataset was created artificially for the Kaggle Playground Series S4E7, based on [this set](https://www.kaggle.com/datasets/annantkumarsingh/health-insurance-cross-sell-prediction-data). For EDA, I'll use only a sample.","metadata":{"id":"2KMUWgUBv7E_"}},{"cell_type":"code","source":"# Paths to datasets\ntrain_path = \"/kaggle/input/playground-series-s4e7/train.csv\"\ntest_path = \"/kaggle/input/playground-series-s4e7/test.csv\"\n\n# # Paths to datasets\n# train_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\train.csv\"\n# test_path = r\"C:\\Users\\paulo\\OneDrive\\Documents\\kaggle_competition_2_datasets\\test.csv\"\n\neda_df = pd.read_csv(train_path).sample(frac=0.005)","metadata":{"id":"DbaTEcD4v7FA","execution":{"iopub.status.busy":"2024-07-30T15:41:33.160817Z","iopub.execute_input":"2024-07-30T15:41:33.161556Z","iopub.status.idle":"2024-07-30T15:41:55.917051Z","shell.execute_reply.started":"2024-07-30T15:41:33.161518Z","shell.execute_reply":"2024-07-30T15:41:55.916098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Displaying Sampled Data\nAfter performing sampling, we display the first few rows of the sampled dataset to understand its structure and verify the sampling process.","metadata":{"id":"755s1SzvvpzE"}},{"cell_type":"code","source":"eda_df.head()","metadata":{"id":"F28JQ120vpzE","outputId":"374debce-58aa-42e5-cd6f-5207eb3ce6b9","execution":{"iopub.status.busy":"2024-07-30T15:41:55.918326Z","iopub.execute_input":"2024-07-30T15:41:55.918664Z","iopub.status.idle":"2024-07-30T15:41:55.948930Z","shell.execute_reply.started":"2024-07-30T15:41:55.918633Z","shell.execute_reply":"2024-07-30T15:41:55.948047Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda_df.info()","metadata":{"id":"87gsLxShvpzF","outputId":"1f49b7a3-1895-4b23-ef7d-2d55411e3204","execution":{"iopub.status.busy":"2024-07-30T15:41:55.952150Z","iopub.execute_input":"2024-07-30T15:41:55.952532Z","iopub.status.idle":"2024-07-30T15:41:55.992775Z","shell.execute_reply.started":"2024-07-30T15:41:55.952506Z","shell.execute_reply":"2024-07-30T15:41:55.991738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Summary\nWe use the `describe` method to generate summary statistics for the numerical columns in the dataset. This provides insights into the central tendency, dispersion, and shape of the datasetâ€™s distribution.\n","metadata":{"id":"a4IAPazUvpzF"}},{"cell_type":"code","source":"eda_df.describe()","metadata":{"id":"TK7MWJZRvpzF","outputId":"160071d9-7cbe-43a1-9622-05d0cbb0d9e9","execution":{"iopub.status.busy":"2024-07-30T15:41:55.994137Z","iopub.execute_input":"2024-07-30T15:41:55.994531Z","iopub.status.idle":"2024-07-30T15:41:56.044992Z","shell.execute_reply.started":"2024-07-30T15:41:55.994489Z","shell.execute_reply":"2024-07-30T15:41:56.043943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Missing Values Check\nIt is essential to check for missing values in the dataset as they can affect the model performance. Here, we count the number of missing values in each column.\n\n","metadata":{"id":"prvyLtCLvpzF"}},{"cell_type":"code","source":"eda_df.isnull().sum()","metadata":{"id":"tCD2ZwOOvpzF","outputId":"dcbc222d-db8f-4fda-e799-7625d5f2ef91","execution":{"iopub.status.busy":"2024-07-30T15:41:56.046550Z","iopub.execute_input":"2024-07-30T15:41:56.046937Z","iopub.status.idle":"2024-07-30T15:41:56.076587Z","shell.execute_reply.started":"2024-07-30T15:41:56.046902Z","shell.execute_reply":"2024-07-30T15:41:56.075547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Initial Observations and Ideas\n\n- **Gender, Driving_License, Regional_Code, Previously_Insured, Vehicle_Age, Vehicle_Damage, Policy_Sales_Channel, and Response** are all categories. I will treat most of them as numerical columns for now, except for Gender, Vehicle_Age, Previously_Insured, and Vehicle_Damage, which I will turn into categories to use in KLIB's streamlined categorical plotting. From previous explorations, I know Driving_License only has 1 negative value, so I will drop it.\n- I will later remap those four categories into numerical columns after EDA as part of the preprocessing for the model.\n- I will MinMax scale Age and Vintage when standardizing because they have a reasonable range for this type of transformation.\n","metadata":{"id":"JU4PQyANvpzF"}},{"cell_type":"markdown","source":"# EDA","metadata":{"id":"A7xp3yhNvpzG"}},{"cell_type":"markdown","source":"I turn some categories I want to plot into category dtype to be compatible with klib, they will automatically switch back to numeric during preprocessing.","metadata":{"id":"pomPFlmsvpzG"}},{"cell_type":"code","source":"# Convert specified columns to categorical\ncategorical_columns = ['Gender', 'Vehicle_Age', 'Previously_Insured']\n\nfor col in categorical_columns:\n    eda_df[col] = eda_df[col].astype('category')\n\n# Convert 'Previously_Insured' column to a categorical type with specific labels\neda_df['Previously_Insured'] = pd.Categorical(eda_df['Previously_Insured'], categories=[0, 1], ordered=True)\neda_df['Previously_Insured'] = eda_df['Previously_Insured'].cat.rename_categories([\"Uninsured\", \"Insured\"])\n","metadata":{"id":"PRR-V_1tvpzG","execution":{"iopub.status.busy":"2024-07-30T15:41:56.078096Z","iopub.execute_input":"2024-07-30T15:41:56.079005Z","iopub.status.idle":"2024-07-30T15:41:56.101339Z","shell.execute_reply.started":"2024-07-30T15:41:56.078969Z","shell.execute_reply":"2024-07-30T15:41:56.100577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualizing Data with KLIB\nUsing KLIB, we create categorical plots to visualize the distribution of categorical features in the dataset. This helps in understanding the balance of different categories within the dataset.","metadata":{"id":"hZPCgQZCvpzG"}},{"cell_type":"code","source":"klib.cat_plot(eda_df)","metadata":{"id":"xPQX1VzGvpzG","outputId":"2fe6f740-1f89-48d7-aeca-352d8afaef9a","execution":{"iopub.status.busy":"2024-07-30T15:41:56.102834Z","iopub.execute_input":"2024-07-30T15:41:56.103285Z","iopub.status.idle":"2024-07-30T15:41:57.917950Z","shell.execute_reply.started":"2024-07-30T15:41:56.103234Z","shell.execute_reply":"2024-07-30T15:41:57.917045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### KLIB Cat Plot Explanation\nThe KLIB categorical plot is an interesting way of visualizing binary variables within a dataset. However, it doesn't translate too well with categorical variables having more than two possible values.\n","metadata":{"id":"f7InUaVIvpzG"}},{"cell_type":"markdown","source":"### Plotting Relationships\n\nWe create a custom function to plot the relationship between categorical variables and the target in a 2x2 single figure.\n","metadata":{"id":"NvsiDkSBvpzG"}},{"cell_type":"code","source":"# Function to plot the relationship between categorical variables and the target in a 2x2 single figure\ndef plot_categorical_vs_target(df, cat_cols, target_col):\n    num_plots = len(cat_cols)\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))\n    palette = [\"#66c2a5\", \"#fc8d62\"]  # Custom palette with exactly two colors\n\n    for ax, col in zip(axes.flatten(), cat_cols):\n        sns.countplot(data=df, x=col, hue=target_col, ax=ax, palette=palette)\n        ax.set_title(f'Relation between {col} and {target_col}')\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot the relationships\ncategorical_columns = ['Gender', 'Vehicle_Age', 'Previously_Insured', 'Vehicle_Damage']\nplot_categorical_vs_target(eda_df, categorical_columns, 'Response')\n","metadata":{"id":"FZ-RNmCHvpzG","outputId":"20c794ff-ee04-4a7a-a7cf-f476d4bb4fcf","execution":{"iopub.status.busy":"2024-07-30T15:41:57.919220Z","iopub.execute_input":"2024-07-30T15:41:57.919561Z","iopub.status.idle":"2024-07-30T15:41:58.989781Z","shell.execute_reply.started":"2024-07-30T15:41:57.919531Z","shell.execute_reply":"2024-07-30T15:41:58.988921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The binary categories were balanced for the most part. Gender's relation to the target doesn't tell us much knowing that there are slightly more Males to begin with. People with newer vehicles are much more likely to insure them. Naturally people that are already insured answered No (who knows if that was a no to switching providers as well). People with no vehicle damage are not very likely to ensure their cars.","metadata":{"id":"Y92-JqhZvpzG"}},{"cell_type":"markdown","source":"### Distribution Plots with KLIB\nWe use KLIB to create distribution plots for several features, including Annual_Premium, Age, Region_Code, Policy_Sales_Channel, and Vintage. This helps in understanding the distribution and identifying potential outliers.\n","metadata":{"id":"8GCjieFivpzG"}},{"cell_type":"code","source":"# Plotting categorical features against the target variable\nklib.dist_plot(eda_df[['Annual_Premium']])\nklib.dist_plot(eda_df[['Age']])\nklib.dist_plot(eda_df[['Region_Code']])\nklib.dist_plot(eda_df[['Policy_Sales_Channel']])\nklib.dist_plot(eda_df[['Vintage']])","metadata":{"id":"BNavUqyPvpzH","outputId":"8c5aff84-7bff-4223-db00-f996b45b2683","execution":{"iopub.status.busy":"2024-07-30T15:41:58.991003Z","iopub.execute_input":"2024-07-30T15:41:58.991305Z","iopub.status.idle":"2024-07-30T15:42:02.495093Z","shell.execute_reply.started":"2024-07-30T15:41:58.991280Z","shell.execute_reply":"2024-07-30T15:42:02.494084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution Plot Analysis\n- **Annual_Premium** is heavily right-skewed with some heavy outliers in larger numbers. It is also bimodally distributed with a large concentration in the lower values and a second concentration near the mean. I might use a standard outlier removal method or a more aggressive quartile method. It might also benefit from log-transformation for its skewness.\n- **Age** is right-skewed, but given the nature of the feature, I will leave it as is and use only a MinMax Scaler during preprocessing.\n- **Region_Code** is actually a category but has many different values. It is clear certain regions are much more favored. In a previous notebook, I tried compiling all of the rarer codes into their own category. I might attempt this again but will need to consider that the rare category must be a number in itself for the model.\n- **Policy_Sales_Channel** is similar to Region_Code and will be treated the same way.\n- **Vintage** is the most normally distributed feature and I will probably scale it with a MinMax Scaler.","metadata":{"id":"H9_sxWxDvpzH"}},{"cell_type":"markdown","source":"### Encoding Categorical Features\n\nHere, we encode the 'Previously_Insured' feature. So that it works properly with the klib correlation plot.","metadata":{"id":"OxqRLt9AvpzH"}},{"cell_type":"code","source":"eda_df['Previously_Insured'] = eda_df['Previously_Insured'].cat.codes","metadata":{"id":"Q6wc9-HGvpzH","execution":{"iopub.status.busy":"2024-07-30T15:42:02.496480Z","iopub.execute_input":"2024-07-30T15:42:02.496809Z","iopub.status.idle":"2024-07-30T15:42:02.502437Z","shell.execute_reply.started":"2024-07-30T15:42:02.496780Z","shell.execute_reply":"2024-07-30T15:42:02.501336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Plot with KLIB\nWe use KLIB to create a correlation plot to identify relationships between the features and the target variable. This helps in understanding which features might be important for the model.\n","metadata":{"id":"apaueOFDvpzH"}},{"cell_type":"code","source":"klib.corr_plot(eda_df, target='Response')","metadata":{"id":"uqH4_W98vpzH","outputId":"eef040d6-5347-4c35-ef24-2079d561ef43","execution":{"iopub.status.busy":"2024-07-30T15:42:02.503691Z","iopub.execute_input":"2024-07-30T15:42:02.503985Z","iopub.status.idle":"2024-07-30T15:42:02.897488Z","shell.execute_reply.started":"2024-07-30T15:42:02.503962Z","shell.execute_reply":"2024-07-30T15:42:02.896564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Correlation Plot Analysis\nThe only feature that seems to have a significant relation towards conversion on the target is whether people are uninsured prior to the call. Based on this I will borrow the features that were engineered in another notebook to create categorical features that capture the relationship between other features and 'Previously_Insured'.\n\n##### Reference: https://www.kaggle.com/code/rohanrao/automl-grand-prix-1st-place-solution","metadata":{"id":"WObG4EPlvpzH"}},{"cell_type":"code","source":"del eda_df","metadata":{"id":"Uu7TKe2vvpzH","execution":{"iopub.status.busy":"2024-07-30T15:42:02.901025Z","iopub.execute_input":"2024-07-30T15:42:02.901347Z","iopub.status.idle":"2024-07-30T15:42:02.905767Z","shell.execute_reply.started":"2024-07-30T15:42:02.901320Z","shell.execute_reply":"2024-07-30T15:42:02.904767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Custom Functions\n\nIn this section, we define custom functions to streamline our data processing and feature engineering tasks. These functions help in optimizing memory usage, handling categorical features, and generating new features from existing ones. Let's walk through each of them.\n\n### Memory Optimization Function\n\nEfficient memory usage is crucial when dealing with large datasets. The `reduce_mem_usage` function iterates through all columns of a dataframe and modifies their data type to reduce memory consumption. This optimization ensures that we can handle the dataset more effectively without running into memory issues.\n\n### Safe Mapping Function\n\nHandling categorical features often involves mapping them to numerical values. The `safe_map` function ensures that we map these categories correctly and log any unknown categories that might be encountered during the process.\n\n### Feature Engineering Function\n\nFeature engineering is a critical step in enhancing model performance. The `feature_engineering` function creates new features by combining existing ones. These new features capture interactions between variables that might be predictive of the target variable.\nWe create new features by capturing the interactions between them and encoding that into categorical combinations. This can provide additional information to the model and improve its performance.\n##### Reference: https://www.kaggle.com/code/rohanrao/automl-grand-prix-1st-place-solution\nhttps://www.kaggle.com/code/khangtran94vn/khang-eda-classification-insurance/notebook","metadata":{"id":"b6ActQqSwHJE"}},{"cell_type":"code","source":"def import_data(path, index_col=None):\n    \"\"\"Import data from a CSV file and optimize memory usage.\"\"\"\n    df = pd.read_csv(path, index_col=index_col)\n    return reduce_mem_usage(df)\n\ndef reduce_mem_usage(df):\n    \"\"\"Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\"\"\"\n    for col in df.columns:\n        col_type = df[col].dtype\n        if isinstance(col_type, pd.IntervalDtype):\n            continue\n\n        if str(col_type)[:3] == 'int':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                df[col] = df[col].astype(np.int64)\n        elif str(col_type)[:5] == 'float':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n    return df\n\ndef feature_engineering(df):\n    \"\"\"Feature engineering on the dataset.\"\"\"\n    # Binning age and converting to categorical labels instead of intervals\n    age_bins = pd.cut(df['Age'], bins=7, labels=False)\n    df['Age_Type'] = age_bins\n    df['Vehicle_Age'] = df['Vehicle_Age'].astype('category').cat.codes\n    df['Vehicle_Damage'] = df['Vehicle_Damage'].astype('category').cat.codes\n    df['Previously_Insured'] = df['Previously_Insured'].astype('category').cat.codes\n\n    df['Age_x_Vehicle_Age'] = df['Age_Type'] * df['Vehicle_Age']\n    df['Age_x_Vehicle_Damage'] = df['Age_Type'] * df['Vehicle_Damage']\n    df['Age_x_Previously_Insured'] = df['Age_Type'] * df['Previously_Insured']\n\n    fac_pre = ['Policy_Sales_Channel', 'Vehicle_Damage', 'Annual_Premium', 'Vintage', 'Age_Type']\n    col_pre = []\n    for i in fac_pre:\n        df['Previously_Insured_x_' + i] = pd.factorize(df['Previously_Insured'].astype(str) + df[i].astype(str))[0]\n        col_pre.append('Previously_Insured_x_' + i)\n\n    fac_pro = fac_pre[1:]\n    col_pro = []\n    for i in fac_pro:\n        df['Policy_Sales_Channel_x_' + i] = pd.factorize(df['Policy_Sales_Channel'].astype(str) + df[i].astype(str))[0]\n        col_pro.append('Policy_Sales_Channel_x_' + i)\n    return df, col_pre, col_pro\n\ndef reduce_mem_usage_np(array):\n    \"\"\"Reduce memory usage of a numpy array by converting its dtype without losing significant precision.\"\"\"\n    for col in range(array.shape[1]):\n        col_data = array[:, col]\n        if np.issubdtype(col_data.dtype, np.floating):\n            col_min = col_data.min()\n            col_max = col_data.max()\n            if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n                array[:, col] = col_data.astype(np.float16)\n            elif col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n                array[:, col] = col_data.astype(np.float32)\n        elif np.issubdtype(col_data.dtype, np.integer):\n            col_min = col_data.min()\n            col_max = col_data.max()\n            if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n                array[:, col] = col_data.astype(np.int8)\n            elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n                array[:, col] = col_data.astype(np.int16)\n            elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n                array[:, col] = col_data.astype(np.int32)\n    return array","metadata":{"id":"mNDTNHkOv7FA","execution":{"iopub.status.busy":"2024-07-30T15:42:02.906969Z","iopub.execute_input":"2024-07-30T15:42:02.907379Z","iopub.status.idle":"2024-07-30T15:42:02.933746Z","shell.execute_reply.started":"2024-07-30T15:42:02.907248Z","shell.execute_reply":"2024-07-30T15:42:02.932778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Combining Datasets\nWe combine the datasets for the feature engineering step and seperate them again after to avoid any data leakage, but the transformations themselves to the data are applied seperately to avoid outliers distorting the test set.","metadata":{"id":"dQjSFjXBxlnv"}},{"cell_type":"code","source":"# Load and optimize data\ntrain_df = import_data(train_path, index_col='id')\ntest_df = import_data(test_path, index_col='id')\n\n# train_df = train_df.sample(frac=0.005)\n# test_df = test_df.sample(frac=0.005)\n\n# Combine train and test datasets for consistent transformation\nfull_df = pd.concat([train_df, test_df], axis=0)","metadata":{"id":"mA6Y60csv7FA","execution":{"iopub.status.busy":"2024-07-30T15:42:02.935040Z","iopub.execute_input":"2024-07-30T15:42:02.935428Z","iopub.status.idle":"2024-07-30T15:42:34.539813Z","shell.execute_reply.started":"2024-07-30T15:42:02.935394Z","shell.execute_reply":"2024-07-30T15:42:34.538780Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert columns to category type\nless = ['Gender', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel']\nfor col in less:\n    full_df[col] = full_df[col].astype('category')\n\n# Apply feature engineering to the combined dataset\nfull_df, col_pre, col_pro = feature_engineering(full_df)","metadata":{"id":"QY9U2ckNv7FB","execution":{"iopub.status.busy":"2024-07-30T15:42:34.541072Z","iopub.execute_input":"2024-07-30T15:42:34.541449Z","iopub.status.idle":"2024-07-30T15:46:09.354261Z","shell.execute_reply.started":"2024-07-30T15:42:34.541416Z","shell.execute_reply":"2024-07-30T15:46:09.353350Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split back into train and test sets\ntrain_df = full_df.iloc[:len(train_df), :]\ntest_df = full_df.iloc[len(train_df):, :]\n\n# Split the training data into training and validation sets\nX = train_df.drop('Response', axis=1)\ny = train_df['Response']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)","metadata":{"id":"fwWdYdeBv7FB","execution":{"iopub.status.busy":"2024-07-30T15:46:09.355466Z","iopub.execute_input":"2024-07-30T15:46:09.355755Z","iopub.status.idle":"2024-07-30T15:46:20.965412Z","shell.execute_reply.started":"2024-07-30T15:46:09.355730Z","shell.execute_reply":"2024-07-30T15:46:20.964415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the ColumnTransformer\ncoltrans = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(sparse_output=False, dtype=np.float32), ['Gender', 'Vehicle_Damage']),\n        ('minmax', MinMaxScaler(), ['Age', 'Region_Code', 'Previously_Insured', 'Policy_Sales_Channel', 'Vintage']),\n        ('ordinal', OrdinalEncoder(categories=[[0, 1, 2]], dtype=np.float32), ['Vehicle_Age']),\n        ('robust', RobustScaler(), ['Annual_Premium']),\n        ('standard', StandardScaler(), ['Age_Type', 'Age_x_Vehicle_Age', 'Age_x_Vehicle_Damage', 'Age_x_Previously_Insured']),\n        ('standard_2', StandardScaler(), col_pre + col_pro),\n    ],\n    remainder='passthrough'  # Keeps columns not specified in transformers\n)\n\n# Fit the transformer on the training data and transform both training and validation sets\nX_train = coltrans.fit_transform(X_train)\nX_valid = coltrans.transform(X_valid)\ntest_df = coltrans.transform(test_df.drop('Response', axis=1))\n\n# Reduce memory usage of transformed data\nX_train = reduce_mem_usage_np(X_train)\nX_valid = reduce_mem_usage_np(X_valid)\ntest_df = reduce_mem_usage_np(test_df)\n\ngc.collect()","metadata":{"id":"E_m6unWdv7FB","execution":{"iopub.status.busy":"2024-07-30T15:46:20.966617Z","iopub.execute_input":"2024-07-30T15:46:20.967652Z","iopub.status.idle":"2024-07-30T15:47:15.599659Z","shell.execute_reply.started":"2024-07-30T15:46:20.967616Z","shell.execute_reply":"2024-07-30T15:47:15.598666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Training and Evaluation\nIn this section, we train our model using LightGBM, XGBoost, and CatBoost, and blend their predictions.","metadata":{"id":"peTs55N3wqu3"}},{"cell_type":"markdown","source":"## Modeling Process\n\nFor this competition, I selected CatBoost, LightGBM, and XGBoost. Each model has unique strengths, making them ideal for capturing different aspects of the data. Blending their predictions leverages these strengths for a more robust solution. Because of the massive size of the dataset I'm saving each model iteration created by each fold, in case something goes wrong during modeling.\n\n- **CatBoost:** Excels with categorical variables.\n- **LightGBM:** Offers speed and efficiency.\n- **XGBoost:** Ensures versatility and robustness.","metadata":{"id":"hcH6n_F7w5yt"}},{"cell_type":"markdown","source":"Most of these values for hyperparameters are based on previous searches using OPTUNA or referencing other notebooks from the competition.\n\nReference: https://www.kaggle.com/code/darkdevil18/0-89698-ps4e7-are-you-insured","metadata":{"id":"34RCu4TDw-Ry"}},{"cell_type":"code","source":"# Calculate the ratio for scale_pos_weight\nratio = len(train_df[train_df['Response'] == 0]) / len(train_df[train_df['Response'] == 1])\nclass_weights = {0: 1, 1: ratio}","metadata":{"id":"UwwIvv50v7FB","execution":{"iopub.status.busy":"2024-07-30T15:47:15.600633Z","iopub.execute_input":"2024-07-30T15:47:15.600893Z","iopub.status.idle":"2024-07-30T15:47:17.143992Z","shell.execute_reply.started":"2024-07-30T15:47:15.600871Z","shell.execute_reply":"2024-07-30T15:47:17.142982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CatBoost\n\n- **Handles Categorical Data Natively:** Reduces preprocessing needs.\n- **Ordered Boosting:** Minimizes overfitting and improves generalization.","metadata":{"id":"P3o8lCy2xDFV"}},{"cell_type":"code","source":"# # Define CatBoost parameters\n# cat_params = {\n#     'loss_function': 'Logloss',\n#     'eval_metric': 'AUC',\n#     'class_names': [0, 1],\n#     'learning_rate': 0.05,\n#     'iterations': 10000,\n#     'depth': 9,\n#     'bagging_temperature': 0.017138393608280057,\n#     'random_strength': 9.256288011643901,\n#     'max_bin': 404,\n#     'l2_leaf_reg': 55.37964307854247,\n#     'task_type': 'GPU',  \n#     'allow_writing_files': False,\n#     'verbose': 500,\n#     'class_weights': class_weights,\n#     'max_leaves': 512,\n#     'fold_permutation_block': 64,\n#     #'thread_count': -1\n# }\n\n# test_pool = Pool(test_df)\n# train_pool = Pool(X_train, y_train)\n# valid_pool = Pool(X_valid, y_valid)\n\n# cbc = CatBoostClassifier(**cat_params)\n# cbc.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=50, verbose=1000)\n\n# test_pred_cbc = cbc.predict_proba(test_pool)[:, 1]\n\n# del cbc, test_pool, train_pool, valid_pool\n\n# gc.collect()","metadata":{"id":"layRBhVRv7FB","execution":{"iopub.status.busy":"2024-07-30T15:47:17.145249Z","iopub.execute_input":"2024-07-30T15:47:17.145539Z","iopub.status.idle":"2024-07-30T15:47:17.151240Z","shell.execute_reply.started":"2024-07-30T15:47:17.145513Z","shell.execute_reply":"2024-07-30T15:47:17.150269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LightGBM\n\n- **Histogram-Based Algorithms:** Efficient training and memory usage.\n- **Leaf-Wise Growth:** Reduces loss more aggressively, enhancing accuracy.","metadata":{"id":"3CBySt1kxFtO"}},{"cell_type":"code","source":"# # Define LightGBM parameters\n# lgb_params = {\n#                'max_depth': 6,\n#                \"metric\": \"auc\",\n#                \"early_stopping_round\": 50,\n#                \"max_bin\": 255,\n#                'num_leaves': 223,\n#                'learning_rate': 0.028095688623590447,\n#                'min_child_samples': 54,\n#                'subsample': 0.5395472919165504,\n#                'colsample_bytree': 0.547518064129546,\n#                'lambda_l1': 3.4444245446562,\n#                'lambda_l2': 2.87490408088595e-05,\n#                'scale_pos_weight': ratio,\n#                'early_stopping_rounds': 10,\n#                'device': 'gpu',\n#                'verbose': -1,\n# #                'n_jobs': -1\n# }\n\n# # Train LightGBM model\n# train_data = lgb.Dataset(X_train, label=y_train)\n# valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n\n\n# lgb = lgb.train(\n#     lgb_params,\n#     train_data,\n#     num_boost_round=2000,\n#     valid_sets=[train_data, valid_data],\n# )\n\n# test_pred_lgb = lgb.predict(test_df, num_iteration=lgb.best_iteration)\n\n# del lgb, train_data, valid_data\n\n# gc.collect()","metadata":{"id":"Ee5ssXN-v7FB","execution":{"iopub.status.busy":"2024-07-30T15:47:17.152591Z","iopub.execute_input":"2024-07-30T15:47:17.153240Z","iopub.status.idle":"2024-07-30T15:47:17.165034Z","shell.execute_reply.started":"2024-07-30T15:47:17.153186Z","shell.execute_reply":"2024-07-30T15:47:17.164125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost\n\n- **Regularization (L1 and L2):** Prevents overfitting, improving robustness.\n- **Tree Pruning:** Effectively manages overfitting.\n- **Parallel Processing:** Speeds up training on large datasets.","metadata":{"id":"6wk1OrDOxHZu"}},{"cell_type":"code","source":"# Define XGBoost parameters\nxgb_params = {\n    'random_state': 512,\n    'objective': \"binary:logistic\",\n    'eval_metric': 'auc',\n    'max_depth': 8,\n    'min_child_weight': 12,\n    'colsample_bytree': 0.5,\n    'gamma': 0.2,\n    'learning_rate': 0.09093568107192034,\n    'subsample': 1.0,\n    'reg_alpha': 0.0011852827097616767,\n    'reg_lambda': 1.0735757602378362e-06,\n    'max_bin': 262143,\n    'scale_pos_weight': ratio,  # Adjust this based on your dataset\n    'tree_method': 'gpu_hist',  # Ensure your environment supports GPU\n    'device': 'cuda',  # Ensure your environment supports GPU\n}\n\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndvalid = xgb.DMatrix(X_valid, label=y_valid)\n\nmodel = xgb.train(\n        xgb_params,\n        dtrain,\n        num_boost_round=10000,\n        evals=[(dtrain, 'train'), (dvalid, 'valid')],\n        verbose_eval=500,\n        early_stopping_rounds=100,\n    )\n\ndtest = xgb.DMatrix(test_df)\ntest_pred_xgb = model.predict(dtest, iteration_range=(0, model.best_iteration))\n\ndel model, dtrain, dvalid, dtest\n\ngc.collect()","metadata":{"id":"VEiyLxR0v7FB","execution":{"iopub.status.busy":"2024-07-30T15:47:17.166147Z","iopub.execute_input":"2024-07-30T15:47:17.166478Z","iopub.status.idle":"2024-07-30T16:01:58.705610Z","shell.execute_reply.started":"2024-07-30T15:47:17.166454Z","shell.execute_reply":"2024-07-30T16:01:58.704594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Blending Predictions\nWe blend predictions from CatBoost, LightGBM, and XGBoost models to create the final submission.","metadata":{"id":"WiyQ537wxJ4r"}},{"cell_type":"code","source":"# # Blending predictions with calculated weights\n# blended_preds = (test_pred_cbc + test_pred_xgb)/2\n\n# Reimport the test_df to get the original index\ntest_df = import_data(test_path, index_col='id')\n\n# Create a submission DataFrame using the original test index\nsubmission = pd.DataFrame({\n    'id': test_df.index,\n    'Response': test_pred_xgb\n})\n\n# Save the submission DataFrame to a CSV file\nsubmission.to_csv('submission.csv', index=False)","metadata":{"id":"JsgMD0oqv7FB","execution":{"iopub.status.busy":"2024-07-30T16:05:53.396230Z","iopub.execute_input":"2024-07-30T16:05:53.396898Z","iopub.status.idle":"2024-07-30T16:06:20.934236Z","shell.execute_reply.started":"2024-07-30T16:05:53.396869Z","shell.execute_reply":"2024-07-30T16:06:20.933164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## What We Did in This Notebook\n\nThis notebook showcased a detailed and comprehensive end-to-end workflow for tackling the Kaggle Playground Series S4E07 competition. Hereâ€™s a recap of our journey:\n\n### Exploratory Data Analysis (EDA)\n- **Initial Explorations:** We kicked off with a subset of the data to get a quick grasp of its structure and nuances. This initial dive helped us spot potential issues and interesting patterns early on.\n- **KLIB Visualizations:** We utilized KLIB for streamlined visualizations and correlation analysis, making it easier to pinpoint key features and understand their relationships with the target variable.\n\n### Data Preprocessing\n- **Handling Missing Values:** We meticulously checked for missing values to ensure data integrity throughout the modeling process.\n- **Categorical Encoding:** By converting categorical variables into numerical formats, we prepped the data for machine learning models.\n- **Memory Optimization:** To efficiently handle the large dataset, we optimized memory usage through downcasting of data types.\n\n### Feature Engineering\n- **Creating New Features:** We got creative with feature engineering, developing new features by combining existing ones. This allowed us to capture interactions between variables that could boost model performance.\n- **Scaling Numerical Features:** Standard scaling was applied to numerical features to bring them onto a similar scale, which is essential for many machine learning algorithms to perform optimally.\n\n### Modeling and Evaluation\n- **Model Selection:** We chose three powerhouse models â€“ CatBoost, LightGBM, and XGBoost â€“ each known for its unique strengths in handling structured data.\n- **Cross-Validation:** To get a reliable estimate of model performance, we used stratified k-fold cross-validation, ensuring our models were tested across diverse subsets of the data.\n- **Hyperparameter Tuning:** Leveraging Optuna, we efficiently tuned the hyperparameters, squeezing out the best performance from our models.\n\n### Blending Predictions\n- **Ensemble Approach:** Instead of relying on a single model, we blended predictions from CatBoost, LightGBM, and XGBoost based on their ROC-AUC scores. This ensemble method harnessed the strengths of each model, resulting in superior overall performance.","metadata":{"id":"miE1HKpVxNBK"}}]}